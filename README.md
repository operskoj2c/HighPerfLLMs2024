# High Performance LLMs 2024
Build a full scale, high-performance LLM from scratch in Jax! We’ll cover training and inference, roofline analysis, compilation, sharding, profiling and more. You’ll leave the class comfortable in Jax and confident in your ability to design high-performance computing systems that reach near their physical limit.

Link to the Discord: [https://discord.gg/e5pYxM3D](https://discord.gg/e5pYxM3D)

# Syllabus. Over 10 lessons we will:
* Build a Jax LLM Implementation From Scratch
* Analyze Single Chip Rooflines And Compilation
* Analyze Distributed Computing via Sharding
* Optimize LLM Training – what happens under the hood, rooflines, sharding
* Optimize LLM Inference – what happens under the hood, rooflines, sharding
* Deep Dive into flash, vLLM, continuous batching, etc.
* Some deep dives along the way:
  - Attention, Flash Attention, vLLM, continuous batching
  - ML: Quantization, Checkpointing, Data Loading, Numerics
  - Practical Tips: Debugging, Overlapping Jax Kernels
  - Larger scale: Goodput
  - Fancy stuff: Ahead of Time Compilation
  - Going deeper: shard map, pallas.

# Approximate Timing
3:30PM Pacific on Wednesdays, starting 2/21/2024. See below for links

# Session Timing, Slides, Videos and Take-Home Exercises

| Session    |              Time                | Link to join                                                 | Slides                           | Take-Home Exercises          |
| --------   | -------                          |  ----                                                        |         -----                    |        -----                 |
| 1          | 3:30PM US Pacific, 2/21/2024     | [meet.google.com/tdd-brrt-gtp](meet.google.com/tdd-brrt-gtp) | To come                          |  To come                     |
| 2          | 3:30PM US Pacific, 2/28/2024     |                                                              |                                  |                              |

About me:
I’m Rafi Witten, a tech lead on Cloud TPU/GPU Multipod. We develop MaxText and aim to push the frontier on Perf/TCO. In 2023, we executed the ["Largest ML Job"](https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e?e=13802955#:~:text=We%20demonstrated%20the%20benefits%20of,JAX%20ML%20framework%2C%20utilizing%20both) ever demonstrated in public and pioneered [“Accurate Quantized Training”](https://cloud.google.com/blog/products/compute/accurate-quantized-training-aqt-for-tpu-v5e?e=13802955), a technique for training with 8-bit integers.

Contact me via Discord [https://discord.gg/e5pYxM3D](https://discord.gg/e5pYxM3D)
